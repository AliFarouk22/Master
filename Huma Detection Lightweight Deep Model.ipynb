{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "github_LW_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliFarouk22/Master/blob/master/Huma%20Detection%20Lightweight%20Deep%20Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDoiX_yoLx6K",
        "colab_type": "text"
      },
      "source": [
        "# **Connect to Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILFG8W_waK5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers, models, losses, activations, optimizers, metrics, regularizers, initializers, callbacks\n",
        "\n",
        "print(tf.VERSION)\n",
        "print(tf.keras.__version__)\n",
        "\n",
        "\n",
        "# from keras import layers, models, losses, activations, optimizers, metrics, regularizers, initializers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# import tensorflow as tf\n",
        "\n",
        "np.random.seed(123)\n",
        "tf.set_random_seed(123)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3qQB216LVHB",
        "colab_type": "text"
      },
      "source": [
        "# **GPU test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIbUIiooLQ3j",
        "colab_type": "code",
        "outputId": "55d3b697-1694-4f74-9a74-1627a95a4aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zAr4pcsL_hX",
        "colab_type": "text"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--yN1rpxMC69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "\n",
        "from skimage import exposure\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "import numpy as np\n",
        "# import tensorflow as tf\n",
        "import random as rn\n",
        "from keras import backend as K\n",
        "\n",
        "from keras import models\n",
        "from keras.utils import CustomObjectScope\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "from keras.applications import VGG16\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# from keras.optimizers import SGD\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
        "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose, SeparableConv2D\n",
        "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
        "from keras.layers.merge import concatenate, add\n",
        "from keras.optimizers import Adam, RMSprop, Nadam\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras import losses\n",
        "from keras.activations import relu\n",
        "from keras.layers import advanced_activations\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95uCk1pidtch",
        "colab_type": "text"
      },
      "source": [
        "# Code to make results ***reproducable***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-APpzrseAQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# The below is necessary for starting Numpy generated random numbers\n",
        "# in a well-defined initial state.\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# The below is necessary for starting core Python generated random numbers\n",
        "# in a well-defined state.\n",
        "\n",
        "rn.seed(1234)\n",
        "\n",
        "# Force TensorFlow to use single thread.\n",
        "# Multiple threads are a potential source of non-reproducible results.\n",
        "# For further details, see: https://stackoverflow.com/questions/42022950/\n",
        "\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
        "                              inter_op_parallelism_threads=1)\n",
        "\n",
        "\n",
        "# The below tf.set_random_seed() will make random number generation\n",
        "# in the TensorFlow backend have a well-defined initial state.\n",
        "# For further details, see:\n",
        "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
        "\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "# Rest of code follows ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTRQPINRMHYI",
        "colab_type": "text"
      },
      "source": [
        "# ***Read full image 224 x 224***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vopqdr94MSJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# train_dir = '/content/drive/My Drive/Master/Dataset/INRIA/Train/train/' ## INRIA \n",
        "# train_dir = '/content/drive/My Drive/Master/Dataset/Stanford40_SomeClasses/' ## Stanford40 \n",
        "# train_dir = '/content/drive/My Drive/Master/Dataset/ImageNet/train/' ## ImageNet \n",
        "train_dir = '/content/drive/My Drive/Master/Dataset/ImgNt_INR_Stan/' ## ALL datasets ImgNet_INR_Stan\n",
        "# train_dir = '/content/drive/My Drive/Master/Dataset/ImgNt_INR_Sample/' ## ImgNt & INR Samples\n",
        "validation_dir = '/content/drive/My Drive/Master/Dataset/INRIA/Train/validation/'\n",
        "\n",
        "\n",
        "def contrast_stretching(img):\n",
        "    p2, p98 = np.percentile(img, (2, 98))\n",
        "    img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98), out_range=(p2, p98))\n",
        "\n",
        "    return img_rescale\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255, \n",
        "                             preprocessing_function=contrast_stretching\n",
        ")\n",
        "\n",
        "batchTrain = 40\n",
        "batchValidation = 40\n",
        "\n",
        "\n",
        "train_generator = datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=batchTrain, class_mode='binary', shuffle=True, seed=1234)#, save_to_dir='G:\\PyCharmWorkspace\\Keras\\\\all\\SavedFromKerasGenerator')\n",
        "validation_generator = datagen.flow_from_directory(validation_dir, target_size=(224, 224), batch_size=batchValidation, class_mode='binary', seed=1234)\n",
        "# test_generator = test_datagen.flow_from_directory(test_dir, target_size=(128, 64), batch_size=740, class_mode='binary')\n",
        "# train_generator = train_datagen.flow(x=, batch_size=32, save_to_dir='')\n",
        "\n",
        "print(\"Generator Sizes: \", train_generator.n, \", \", validation_generator.n)\n",
        "print(\"Batch Sizes: \", train_generator.batch_size, \", \", validation_generator.batch_size)\n",
        "print(\"Number of Classes: \", train_generator.num_classes, \", \", validation_generator.num_classes)\n",
        "# print(\"classes: \", train_generator[0])\n",
        "# print(\"classes: \", train_generator.shuffle)\n",
        "\n",
        "print(train_generator[0][0].shape)\n",
        "img = train_generator[0][0][10]\n",
        "print(\"===================\", np.max(img) , type(img[0][0][0]), img.mean())\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4tdu4HjBYpR",
        "colab_type": "text"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQd9A9z1VqNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dropout=0.35\n",
        "mySeed = 1234\n",
        "initializer = initializers.glorot_uniform(seed=mySeed)#truncated_normal(mean=0.0, stddev=0.5, seed=mySeed)#random_normal(mean=0.0, stddev=0.01, seed=mySeed)\n",
        "l2Value = 0.001\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(7, 7), #activation='relu',\n",
        "                        input_shape=(224, 224, 3),\n",
        "                        kernel_regularizer=regularizers.l2(l2Value),\n",
        "                        strides=(1, 1), padding='same',\n",
        "                        kernel_initializer=initializer\n",
        "                       ))\n",
        "model.add(layers.BatchNormalization(axis=3))\n",
        "model.add(layers.Activation('relu'))\n",
        "# model.add(layers.activations.relu())\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(dropout, seed=mySeed))\n",
        "\n",
        "#bottleneck\n",
        "# model.add(layers.Conv2D(filters=20, kernel_size=(1, 1),\n",
        "#                         kernel_regularizer=regularizers.l2(l2Value),\n",
        "#                         strides=(1, 1), padding='same',\n",
        "#                         kernel_initializer=initializer\n",
        "#                         ))\n",
        "# model.add(layers.Activation('relu'))\n",
        "\n",
        "model.add(layers.SeparableConv2D(filters=64, kernel_size=(5, 5),# activation='relu',\n",
        "                                 strides=(1, 1),\n",
        "                                 padding='same', data_format='channels_last', depth_multiplier=1,\n",
        "                                 depthwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 pointwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 depthwise_initializer=initializer, \n",
        "                                 pointwise_initializer=initializer\n",
        "                                ))\n",
        "model.add(layers.BatchNormalization(axis=3))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(dropout, seed=mySeed))\n",
        "\n",
        "#bottleneck\n",
        "# model.add(layers.Conv2D(filters=16, kernel_size=(1, 1),\n",
        "#                         kernel_regularizer=regularizers.l2(l2Value),\n",
        "#                         strides=(1, 1), padding='same',\n",
        "#                         kernel_initializer=initializer\n",
        "#                         ))\n",
        "# model.add(layers.Activation('relu'))\n",
        "\n",
        "model.add(layers.SeparableConv2D(filters=128, kernel_size=(3, 3),# activation='relu',\n",
        "                                 strides=(1, 1),\n",
        "                                 padding='same', data_format='channels_last', depth_multiplier=1,\n",
        "                                 depthwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 pointwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 depthwise_initializer=initializer, \n",
        "                                 pointwise_initializer=initializer\n",
        "                                ))\n",
        "model.add(layers.BatchNormalization(axis=3))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(dropout, seed=mySeed))\n",
        "\n",
        "#bottleneck\n",
        "# model.add(layers.Conv2D(filters=16, kernel_size=(1, 1),\n",
        "#                         kernel_regularizer=regularizers.l2(l2Value),\n",
        "#                         strides=(1, 1), padding='same',\n",
        "#                         kernel_initializer=initializer\n",
        "#                         ))\n",
        "# model.add(layers.Activation('relu'))\n",
        "\n",
        "model.add(layers.SeparableConv2D(filters=128, kernel_size=(3, 3),# activation='relu',\n",
        "                                 strides=(1, 1),\n",
        "                                 padding='same', data_format='channels_last', depth_multiplier=1,\n",
        "                                 depthwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 pointwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 depthwise_initializer=initializer, \n",
        "                                 pointwise_initializer=initializer\n",
        "                                ))\n",
        "model.add(layers.BatchNormalization(axis=3))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(dropout, seed=mySeed))\n",
        "\n",
        "#bottleneck\n",
        "# model.add(layers.Conv2D(filters=32, kernel_size=(1, 1),\n",
        "#                         kernel_regularizer=regularizers.l2(l2Value),\n",
        "#                         strides=(1, 1), padding='same',\n",
        "#                         kernel_initializer=initializer\n",
        "#                         ))\n",
        "# model.add(layers.Activation('relu'))\n",
        "\n",
        "model.add(layers.SeparableConv2D(filters=512, kernel_size=(3, 3),# activation='relu',\n",
        "                                 strides=(1, 1),\n",
        "                                 padding='same', data_format='channels_last', depth_multiplier=1,\n",
        "                                 depthwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 pointwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 depthwise_initializer=initializer, \n",
        "                                 pointwise_initializer=initializer\n",
        "                                ))\n",
        "model.add(layers.BatchNormalization(axis=3))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(dropout, seed=mySeed))\n",
        "\n",
        "#bottleneck\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=(1, 1),\n",
        "                        kernel_regularizer=regularizers.l2(l2Value),\n",
        "                        strides=(1, 1), padding='same',\n",
        "                        kernel_initializer=initializer\n",
        "                        ))\n",
        "model.add(layers.Activation('relu'))\n",
        "\n",
        "# 7*7 \n",
        "model.add(layers.SeparableConv2D(filters=512, kernel_size=(7, 7),# activation='relu',\n",
        "                                 strides=(1, 1),\n",
        "                                 padding='valid', data_format='channels_last', depth_multiplier=1,\n",
        "                                 depthwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 pointwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 depthwise_initializer=initializer, \n",
        "                                 pointwise_initializer=initializer\n",
        "                                ))\n",
        "model.add(layers.BatchNormalization(axis=3))\n",
        "model.add(layers.Activation('relu'))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "# model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2Value) , kernel_initializer=initializer))\n",
        "# model.add(layers.Dropout(dropout, seed=mySeed))\n",
        "model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=initializer))\n",
        "\n",
        "print( model.summary() )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmafemOYgUyu",
        "colab_type": "text"
      },
      "source": [
        "# **Model 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6Gbv_ESjvGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dropout=0.35\n",
        "mySeed = 1234\n",
        "initializer = initializers.glorot_uniform(seed=mySeed)#truncated_normal(mean=0.0, stddev=0.5, seed=mySeed)#random_normal(mean=0.0, stddev=0.01, seed=mySeed)\n",
        "l2Value = 0.001\n",
        "\n",
        "inputLayer = layers.Input(shape=(224, 224, 3))\n",
        "\n",
        "layer = Conv2D(filters=64, kernel_size=(3, 3), #activation='relu',\n",
        "                        kernel_regularizer=regularizers.l2(l2Value),\n",
        "                        strides=(1, 1), padding='same',\n",
        "                        kernel_initializer=initializer\n",
        "                       ) (inputLayer)\n",
        "layer = Activation('relu') (layer)\n",
        "layer = Conv2D(filters=64, kernel_size=(3, 3), #activation='relu',\n",
        "                        kernel_regularizer=regularizers.l2(l2Value),\n",
        "                        strides=(1, 1), padding='same',\n",
        "                        kernel_initializer=initializer\n",
        "                       ) (layer)\n",
        "layer = Activation('relu') (layer)\n",
        "layer = Conv2D(filters=64, kernel_size=(3, 3), #activation='relu',\n",
        "                        kernel_regularizer=regularizers.l2(l2Value),\n",
        "                        strides=(1, 1), padding='same',\n",
        "                        kernel_initializer=initializer\n",
        "                       ) (layer)\n",
        "layer = BatchNormalization() (layer)\n",
        "layer = Activation('relu') (layer)\n",
        "layer = MaxPooling2D((2, 2)) (layer)\n",
        "layer = Dropout(dropout, seed=mySeed) (layer)\n",
        "\n",
        "layer = SeparableConv2D(filters=64, kernel_size=(3, 3),# activation='relu',\n",
        "                                 strides=(1, 1),\n",
        "                                 padding='same', data_format='channels_last', depth_multiplier=1,\n",
        "                                 depthwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 pointwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 depthwise_initializer=initializer, \n",
        "                                 pointwise_initializer=initializer\n",
        "                                ) (layer)\n",
        "layer = Activation('relu') (layer)\n",
        "layer = SeparableConv2D(filters=64, kernel_size=(3, 3),# activation='relu',\n",
        "                                 strides=(1, 1),\n",
        "                                 padding='same', data_format='channels_last', depth_multiplier=1,\n",
        "                                 depthwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 pointwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 depthwise_initializer=initializer, \n",
        "                                 pointwise_initializer=initializer\n",
        "                                ) (layer)\n",
        "layer = BatchNormalization() (layer)\n",
        "layer = Activation('relu') (layer)\n",
        "layer = MaxPooling2D((2, 2)) (layer)\n",
        "layer = Dropout(dropout, seed=mySeed) (layer)\n",
        "\n",
        "layer = SeparableConv2D(filters=128, kernel_size=(3, 3),# activation='relu',\n",
        "                                 strides=(1, 1),\n",
        "                                 padding='same', data_format='channels_last', depth_multiplier=1,\n",
        "                                 depthwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 pointwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 depthwise_initializer=initializer, \n",
        "                                 pointwise_initializer=initializer\n",
        "                                ) (layer)\n",
        "layer = BatchNormalization() (layer)\n",
        "layer = Activation('relu') (layer)\n",
        "layer = MaxPooling2D((2, 2)) (layer)\n",
        "layer = Dropout(dropout, seed=mySeed) (layer)\n",
        "\n",
        "layer = SeparableConv2D(filters=128, kernel_size=(3, 3),# activation='relu',\n",
        "                                 strides=(1, 1),\n",
        "                                 padding='same', data_format='channels_last', depth_multiplier=1,\n",
        "                                 depthwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 pointwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 depthwise_initializer=initializer, \n",
        "                                 pointwise_initializer=initializer\n",
        "                                ) (layer)\n",
        "layer = BatchNormalization() (layer)\n",
        "layer = Activation('relu') (layer)\n",
        "layer = MaxPooling2D((2, 2)) (layer)\n",
        "layer = Dropout(dropout, seed=mySeed) (layer)\n",
        "\n",
        "layer = SeparableConv2D(filters=512, kernel_size=(3, 3),# activation='relu',\n",
        "                                 strides=(1, 1),\n",
        "                                 padding='same', data_format='channels_last', depth_multiplier=1,\n",
        "                                 depthwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 pointwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 depthwise_initializer=initializer, \n",
        "                                 pointwise_initializer=initializer\n",
        "                                ) (layer)\n",
        "layer = BatchNormalization() (layer)\n",
        "layer = Activation('relu') (layer)\n",
        "layer = MaxPooling2D((2, 2)) (layer)\n",
        "layer = Dropout(dropout, seed=mySeed) (layer)\n",
        "\n",
        "#bottleneck\n",
        "layer = Conv2D(filters=128, kernel_size=(1, 1),\n",
        "                        kernel_regularizer=regularizers.l2(l2Value),\n",
        "                        strides=(1, 1), padding='same',\n",
        "                        kernel_initializer=initializer\n",
        "                        ) (layer)\n",
        "layer = Activation('relu') (layer)\n",
        "\n",
        "# 7*7 \n",
        "layer = SeparableConv2D(filters=512, kernel_size=(7, 7),# activation='relu',\n",
        "                                 strides=(1, 1),\n",
        "                                 padding='valid', data_format='channels_last', depth_multiplier=1,\n",
        "                                 depthwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 pointwise_regularizer=regularizers.l2(l2Value),\n",
        "                                 depthwise_initializer=initializer, \n",
        "                                 pointwise_initializer=initializer\n",
        "                                ) (layer)\n",
        "layer = BatchNormalization() (layer)\n",
        "layer = Activation('relu') (layer)\n",
        "\n",
        "layer = layers.Flatten() (layer)\n",
        "outputs = Dense(1, activation='sigmoid', kernel_initializer=initializer) (layer)\n",
        "\n",
        "model = Model(inputs=[inputLayer], outputs=[outputs])\n",
        "\n",
        "print( model.summary() )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72yVot74QHQO",
        "colab_type": "text"
      },
      "source": [
        "# **Compile Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcYEjdYmQLjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=optimizers.Adam(lr=0.0007),# decay=0.001),\n",
        "              loss=losses.binary_crossentropy,\n",
        "              metrics=[metrics.binary_accuracy]\n",
        "             )\n",
        "\n",
        "\n",
        "print( model.summary() )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHfPwvjxWvE5",
        "colab_type": "text"
      },
      "source": [
        "# **VGG layers weights to model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaeD34dhW0xL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(224, 224, 3))\n",
        "\n",
        "conv_base.summary()\n",
        "\n",
        "model.summary()\n",
        "\n",
        "print('My Model weights: ', type(model.get_weights()), len(model.get_weights()))\n",
        "print('VGG16 Model weights: ', type(conv_base.get_weights()), len(conv_base.get_weights()))\n",
        "\n",
        "vggWeights = np.empty(0)\n",
        "\n",
        "for vggLayer in conv_base.layers:\n",
        "\n",
        "  if \"conv\" in vggLayer.name:\n",
        "  \n",
        "    vggModelLayerWeightsFlattened = vggLayer.get_weights()[0].flatten()\n",
        "    print(\"Flattened VGG Weights Shape: \", vggModelLayerWeightsFlattened.shape, \", \", vggLayer.get_weights()[0].shape)\n",
        "    vggWeights = np.concatenate((vggWeights, vggModelLayerWeightsFlattened), axis=None)\n",
        "    \n",
        "print(\"VGG weights flattened appended Length: \", vggWeights.shape)\n",
        "\n",
        "i = 0\n",
        "\n",
        "for lay in model.layers:\n",
        "  \n",
        "  print(lay.name)\n",
        "  lastStop = 0\n",
        "  \n",
        "  if \"conv\" in lay.name:\n",
        "    \n",
        "    vggWeightsToModelWeights = []\n",
        "    numWeightsArraysInLayer = len(lay.get_weights())\n",
        "    print(\"numWeightsArraysInLayer: \", numWeightsArraysInLayer)\n",
        "    \n",
        "    for j in range(numWeightsArraysInLayer):\n",
        "      layerSize = lay.get_weights()[j].shape\n",
        "      print(\"layerSize: \", layerSize)\n",
        "      layerShapeMultiplied = 1\n",
        "      \n",
        "      for dim in layerSize:\n",
        "        layerShapeMultiplied *= dim\n",
        "      \n",
        "      x = vggWeights[lastStop: (lastStop + layerShapeMultiplied)].reshape(layerSize)\n",
        "      lastStop = lastStop + layerShapeMultiplied\n",
        "      vggWeightsToModelWeights.append(x)\n",
        "      \n",
        "    print (\"Sizes of vggWeightsToModelWeights: \", len(vggWeightsToModelWeights))\n",
        "    for weights in vggWeightsToModelWeights:\n",
        "      print(len(weights))\n",
        "      \n",
        "    model.layers[i].set_weights(vggWeightsToModelWeights)\n",
        "    \n",
        "  i += 1\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCrq1eX7tkH7",
        "colab_type": "text"
      },
      "source": [
        "# **Run Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PlmgQSNtniP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print( model.summary() )\n",
        "callbacks = [\n",
        "    # EarlyStopping(patience=3, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.9, patience=10, min_lr=0.0001, verbose=1),\n",
        "    # ReduceLROnPlateau(factor=0.01, patience=3, min_lr=0.0001, verbose=1),\n",
        "    ModelCheckpoint(\"/content/drive/My Drive/Master/INRIA_images_hog/MyIdea/my_keras_model_bottleneck_103_1.h5\",\n",
        "                                          save_best_only=True)#, save_weights_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch=train_generator.n/batchTrain, #((train_generator.n * transformationNumber)/batch),\n",
        "                              epochs=100,\n",
        "                              validation_data=validation_generator,\n",
        "                              validation_steps=(validation_generator.n/batchValidation), \n",
        "                              callbacks=callbacks)\n",
        "\n",
        "\n",
        "acc = history.history['binary_accuracy']\n",
        "val_acc = history.history['val_binary_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcEIKe6lb4Ax",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC_9QiyLGGs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = models.load_model('/content/drive/My Drive/Master/INRIA_images_hog/MyIdea/my_keras_model_bottleneck_103_1.h5')#my_keras_model_bottleneck_9.h5')\n",
        "\n",
        "plot_model(model, to_file='/content/drive/My Drive/Master/INRIA_images_hog/CNN_Models/model.png')\n",
        "\n",
        "# test_dir = '/content/drive/My Drive/Master/Dataset/INRIA/Test/test/'\n",
        "test_dir_pos = '/content/drive/My Drive/Master/Dataset/INRIA/Test/test/pos/' #AllData/test/pos/'\n",
        "test_dir_neg = '/content/drive/My Drive/Master/Dataset/INRIA/Test/test/neg/' #AllData/test/neg/'\n",
        "# test_dir_pos = '/content/drive/My Drive/Master/Dataset/GeneratedTest/allImages/'#workingPos#allImages/' #AllData/test/pos/'\n",
        "# test_dir_pos = '/content/drive/My Drive/Master/Dataset/GeneratedTest/Ali_and_Ahmed/'\n",
        "\n",
        "\n",
        "test_generator_pos = datagen.flow_from_directory(test_dir_pos, target_size=(224, 224), batch_size=1, class_mode='binary', seed=1234)\n",
        "print(\"classes: \", test_generator_pos.classes)\n",
        "test_generator_neg = datagen.flow_from_directory(test_dir_neg, target_size=(224, 224), batch_size=1, class_mode='binary', seed=1234)\n",
        "print(\"classes: \", test_generator_neg.classes)\n",
        "\n",
        "pos11 = np.round( model.predict_generator(test_generator_pos, steps=test_generator_pos.n) )\n",
        "pos1 = np.mean( np.round( model.predict_generator(test_generator_pos, steps=test_generator_pos.n) ) )\n",
        "neg1 = 1 - np.mean( np.round( model.predict_generator(test_generator_neg, steps=test_generator_neg.n) ) )\n",
        "\n",
        "pos11 = np.reshape(pos11, newshape=(1, -1))\n",
        "print(\"__________________________________ SHAPE: \", pos11.shape)\n",
        "names = np.asarray(test_generator_pos.filenames)\n",
        "for index in range(pos11.shape[1]):\n",
        "  if pos11[0, index] == 1.0:\n",
        "    imageName = names[index]\n",
        "    print (\"index: \", index, \"     prediction: \", pos11[0, index], \"     imageName: \", imageName)\n",
        "    # imagePath = test_dir_pos + imageName\n",
        "    # shutil.copy((imagePath) , (\"/content/drive/My Drive/Master/Dataset/GeneratedTest/Ok3/\" + imageName))\n",
        "#     # copy( (imagePath) , (\"/content/drive/My Drive/Master/Dataset/GeneratedTest/Ok/\" + imageName) )Ali_and_Ahmed2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print( \"pos: \", pos11, \"\\n\", np.sum(pos11) ) \n",
        "print( \"pos: \", pos1 ) \n",
        "print( \"neg: \", neg1 )\n",
        "\n",
        "print( \"True pos: \", pos1 ) \n",
        "print( \"True neg: \", neg1 )\n",
        "print( \"False pos:\", 1 - neg1 )\n",
        "print( \"False neg:\", 1 - pos1 )\n",
        "\n",
        "# Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
        "# â€¢\tF-score = 2TP / (2TP + FP + FN)\n",
        "\n",
        "totalAccuracy = (pos1 + neg1) / (pos1 + neg1 + (1-neg1) + (1-neg1))#(pos * (test_generator_pos.n/(test_generator_pos.n + test_generator_neg.n)) ) + (neg * (test_generator_neg.n/(test_generator_pos.n + test_generator_neg.n)) )\n",
        "F_Score = (2*pos1) / ( (2*pos1) + (1-neg1) + (1-pos1) )\n",
        "print(\"Total Accuracy: \", totalAccuracy, \"F-Score\", F_Score)\n",
        "\n",
        "################################################\n",
        "\n",
        "\n",
        "print (\"#####-------------------- Test on ImageNet -------------------#####\")\n",
        "test_dir_pos_ImageNet = '/content/drive/My Drive/Master/Dataset/ImageNet/test/pos/' #AllData/test/pos/'\n",
        "test_dir_neg_ImageNet = '/content/drive/My Drive/Master/Dataset/ImageNet/test/neg/' #AllData/test/neg/'\n",
        "\n",
        "test_generator_pos = datagen.flow_from_directory(test_dir_pos_ImageNet, target_size=(224, 224), batch_size=1, class_mode='binary')\n",
        "print(\"classes: \", test_generator_pos.classes)\n",
        "test_generator_neg = datagen.flow_from_directory(test_dir_neg_ImageNet, target_size=(224, 224), batch_size=1, class_mode='binary')\n",
        "print(\"classes: \", test_generator_neg.classes)\n",
        "\n",
        "\n",
        "pos2 = np.mean( np.round( model.predict_generator(test_generator_pos, steps=test_generator_pos.n) ) )\n",
        "neg2 = 1 - np.mean( np.round( model.predict_generator(test_generator_neg, steps=test_generator_neg.n) ) )\n",
        "\n",
        "\n",
        "print( \"pos: \", pos2 ) \n",
        "print( \"neg: \", neg2 )\n",
        "\n",
        "print( \"True pos: \", pos2 ) \n",
        "print( \"True neg: \", neg2 )\n",
        "print( \"False pos:\", 1 - neg2 )\n",
        "print( \"False neg:\", 1 - pos2 )\n",
        "\n",
        "\n",
        "totalAccuracy = (pos2 + neg2) / (pos2 + neg2 + (1-neg2) + (1-neg2))#(pos * (test_generator_pos.n/(test_generator_pos.n + test_generator_neg.n)) ) + (neg * (test_generator_neg.n/(test_generator_pos.n + test_generator_neg.n)) )\n",
        "F_Score = (2*pos2) / ( (2*pos2) + (1-neg2) + (1-pos2) )\n",
        "print(\"Total Accuracy: \", totalAccuracy, \"F-Score\", F_Score)\n",
        "\n",
        "# ##################################\n",
        "\n",
        "print (\"#####-------------------- Test on Pascal Voc 2007 -------------------#####\")\n",
        "test_dir_pos_ImageNet = '/content/drive/My Drive/Master/Dataset/PascalVoc2007/test/pos/' #AllData/test/pos/'\n",
        "\n",
        "test_generator_pos = datagen.flow_from_directory(test_dir_pos_ImageNet, target_size=(224, 224), batch_size=1, class_mode='binary')\n",
        "print(\"classes: \", test_generator_pos.classes)\n",
        "\n",
        "\n",
        "pos3 = np.mean( np.round( model.predict_generator(test_generator_pos, steps=test_generator_pos.n) ) )\n",
        "\n",
        "\n",
        "print( \"pos: \", pos3 ) \n",
        "\n",
        "print( \"True pos: \", pos3 ) \n",
        "print( \"False neg:\", 1 - pos3 )\n",
        "\n",
        "# F-Score = (2*pos) / ( (2*pos) + (1-neg) + (1-pos))\n",
        "\n",
        "print (\"#####-------------------- Test on All Datasets -------------------#####\")\n",
        "\n",
        "totalPos = (pos1 + pos2 + pos3) / 3\n",
        "totalNeg = (neg1 + neg2) / 2\n",
        "\n",
        "\n",
        "print( \"total pos: \", totalPos ) \n",
        "print( \"total neg: \", totalNeg )\n",
        "\n",
        "print( \"True pos: \", totalPos ) \n",
        "print( \"True neg: \", totalNeg )\n",
        "print( \"False pos:\", 1 - totalNeg )\n",
        "print( \"False neg:\", 1 - totalPos )\n",
        "\n",
        "\n",
        "totalAccuracy = (totalPos + totalNeg) / (totalPos + totalNeg + (1-totalNeg) + (1-totalNeg))#(pos * (test_generator_pos.n/(test_generator_pos.n + test_generator_neg.n)) ) + (neg * (test_generator_neg.n/(test_generator_pos.n + test_generator_neg.n)) )\n",
        "F_Score = (2*totalPos) / ( (2*totalPos) + (1-totalNeg) + (1-totalPos) )\n",
        "print(\"Total Accuracy: \", totalAccuracy, \"F-Score\", F_Score)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQtTrp_6ToGH",
        "colab_type": "text"
      },
      "source": [
        "# **------------------------------------**"
      ]
    }
  ]
}